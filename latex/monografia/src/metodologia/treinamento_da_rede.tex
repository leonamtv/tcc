\section{Treinamento da rede}
\label{sec:train}
\index{Treinamento da rede} 
\index{Backpropagation}

Para que redes neurais aprendam, estas precisam ser submetidas a um processo de treinamento, que no caso de redes neurais, representa uma técnica de ajuste dos pesos sinápticos, a fim de reduzirmos o erro da rede. Existem várias formas de treinar uma rede neural, e uma das mais famosas é a retro propagação do erro, conhecida popularmente pelo termo em inglês \textit{backpropagation} \cite{rumelhart_learning_1986}. Como o nome sugere, o treinamento é realizado perante à disseminação do erro obtido pela rede, de trás pra frente. Isto é, para ajustar os pesos da rede utilizando este algoritmo, insere-se na rede uma entrada da qual a saída esperada é conhecida. Calcula-se então, a diferença entre a saída esperada e obtida. Os pesos em seguida, sãp atualizados proporcionalmente a este erro, levando também em consideração o quanto cada peso interfere no erro, da camada mais profunda, para a camada menos profunda \cite{rumelhart_learning_1986}. A fim de se otimizar este treinamento, existem ferramentas matemáticas para atualizarmos os pesos de formas mais eficientes.

Em 1944, um matemático chamado Haskell B. Curry, descreveu a aplicação de um método já conhecido para a otimização de problemas, que posteriormente ficou conhecido como \textit{Gradient Descent} \cite{curry_method_1944}. Este método utiliza um conceito matemático chamado gradiente, para encontrar a direção mais eficiente para o mínimo de uma função. O gradiente é um vetor de \textit{n} componentes, onde \textit{n} é a dimensão da função a ser otimizada, que no caso do treinamento, é a função que representa o erro da rede neural, em função de todos seus parâmetros treináveis, como seus pesos. Quando o gradiente desta função é calculado, sabe-se, exatamente em qual direção os erros devem ser atualizado para se alcançar o mínimo, o mais rápido possível (com o menor número de iterações de treinamento), diminuindo assim, a complexidade e o tempo necessários para o treinamento.

Retro-propagação do erro em conjunto com o \textit{Gradient Descent} compõem o algoritmo ótimo de treinamento. Com a retro-propagação, altera-se o peso individual, proporcionalmente ao quanto este, interfere no erro. O gradiente então, fornece o caminho mais rápido para minimizar este erro. Para que estas operações sejam realizadas, uma série de cálculos matemáticos são realizadas a cada etapa de treinamento, fazendo com que a a complexidade desta fase crucial no desenvolvimento de redes neurais seja uma das mais demoradas e computacionalmente exaustivas.

Apesar de o tópico de treinamento de redes neurais ser de suma importância nos estudos de inteligência artificial, é externo ao objetivo deste trabalho, descrever em fino detalhe tal atividade. As bibliotecas de inteligência artificial amplamente utilizadas na indústria, que serão utilizadas na parte prática do presente trabalho, abstraem grande parte da complexidade matemática e algebraica do treinamento. Portanto, maiores detalhes sobre treinamento de redes neurais serão omitidos.
