
@misc{wang_real-esrgan_2021,
	title = {Real-{ESRGAN}: {Training} {Real}-{World} {Blind} {Super}-{Resolution} with {Pure} {Synthetic} {Data}},
	shorttitle = {Real-{ESRGAN}},
	url = {http://arxiv.org/abs/2107.10833},
	abstract = {Though many attempts have been made in blind superresolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Speciﬁcally, a high-order degradation modeling process is introduced to better simulate complex realworld degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efﬁcient implementations to synthesize training pairs on the ﬂy.},
	language = {en},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Wang, Xintao and Xie, Liangbin and Dong, Chao and Shan, Ying},
	month = aug,
	year = {2021},
	note = {arXiv:2107.10833 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: Tech Report. Training/testing codes and executable files are in https://github.com/xinntao/Real-ESRGAN},
	file = {Wang et al. - 2021 - Real-ESRGAN Training Real-World Blind Super-Resol.pdf:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/8EYV996C/Wang et al. - 2021 - Real-ESRGAN Training Real-World Blind Super-Resol.pdf:application/pdf},
}

@misc{cancer_imaging_archive_cancer_2022,
	title = {Cancer {Imaging} {Archive}},
	url = {https://wiki.cancerimagingarchive.net/display/NBIA/Downloading+Images+Using+the+NBIA+Data+Retriever},
	urldate = {2022-06-06},
	author = {{Cancer Imaging Archive}},
	month = jun,
	year = {2022},
	file = {Downloading Images Using the NBIA Data Retriever - TCIA Online Help - Cancer Imaging Archive Wiki:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/4GEP9IRZ/Downloading+Images+Using+the+NBIA+Data+Retriever.html:text/html},
}

@book{haykin_redes_2007,
	title = {Redes {Neurais}: {Princípios} e {Prática}},
	isbn = {978-85-7780-086-5},
	shorttitle = {Redes {Neurais}},
	abstract = {As redes neurais artificiais têm raízes em disciplinas como neurociência, matemática, estatística, física, ciência da computação e engenharia. Suas aplicações podem ser encontradas em campos tão diversos quanto modelagem, análise de séries temporais, reconhecimento de padrões, processamento de sinais e controle. Este livro fornece as bases para o entendimento das redes neurais, reconhecendo a natureza multidisciplinar do tema.},
	language = {pt},
	publisher = {Bookman Editora},
	author = {Haykin, Simon},
	year = {2007},
	note = {Google-Books-ID: bhMwDwAAQBAJ},
	keywords = {Computers / Enterprise Applications / Business Intelligence Tools, Computers / Intelligence (AI) \& Semantics},
}

@article{curry_method_1944,
	title = {The method of steepest descent for non-linear minimization problems},
	volume = {2},
	issn = {0033-569X, 1552-4485},
	url = {http://www.ams.org/qam/1944-02-03/S0033-569X-1944-10667-3/},
	doi = {10.1090/qam/10667},
	language = {en},
	number = {3},
	urldate = {2021-09-07},
	journal = {Quarterly of Applied Mathematics},
	author = {Curry, Haskell B.},
	month = oct,
	year = {1944},
	pages = {258--261},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	language = {en},
	number = {6088},
	urldate = {2021-09-07},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536},
}

@article{dong_image_2015,
	title = {Image {Super}-{Resolution} {Using} {Deep} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1501.00092},
	abstract = {We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.},
	urldate = {2021-09-06},
	journal = {arXiv:1501.00092 [cs]},
	author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
	month = jul,
	year = {2015},
	note = {arXiv: 1501.00092},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, I.2.6, I.4.5},
	annote = {Comment: 14 pages, 14 figures, journal},
	file = {arXiv Fulltext PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/E72LJZMG/Dong et al. - 2015 - Image Super-Resolution Using Deep Convolutional Ne.pdf:application/pdf;arXiv.org Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/TG3D9RWM/1501.html:text/html},
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2021-09-06},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/3I4B8Z8Y/Ioffe e Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/HX8LI72S/1502.html:text/html},
}

@article{nasrollahi_super-resolution_2014,
	title = {Super-resolution: a comprehensive survey},
	volume = {25},
	issn = {0932-8092, 1432-1769},
	shorttitle = {Super-resolution},
	url = {http://link.springer.com/10.1007/s00138-014-0623-4},
	doi = {10.1007/s00138-014-0623-4},
	language = {en},
	number = {6},
	urldate = {2021-09-06},
	journal = {Machine Vision and Applications},
	author = {Nasrollahi, Kamal and Moeslund, Thomas B.},
	month = aug,
	year = {2014},
	pages = {1423--1468},
	file = {Versão submetida:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/G3K9YICS/Nasrollahi e Moeslund - 2014 - Super-resolution a comprehensive survey.pdf:application/pdf},
}

@article{wang_esrgan_2018,
	title = {{ESRGAN}: {Enhanced} {Super}-{Resolution} {Generative} {Adversarial} {Networks}},
	shorttitle = {{ESRGAN}},
	url = {http://arxiv.org/abs/1809.00219},
	abstract = {The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at https://github.com/xinntao/ESRGAN .},
	urldate = {2021-09-06},
	journal = {arXiv:1809.00219 [cs]},
	author = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Loy, Chen Change and Qiao, Yu and Tang, Xiaoou},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.00219},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ECCV 2018 workshop. Won Region 3 in the PIRM2018-SR Challenge. Code and models are at https://github.com/xinntao/ESRGAN},
	file = {arXiv Fulltext PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/3A944KFE/Wang et al. - 2018 - ESRGAN Enhanced Super-Resolution Generative Adver.pdf:application/pdf;arXiv.org Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/QUF6NVJH/1809.html:text/html},
}

@misc{noauthor_bash_nodate,
	title = {Bash - {GNU} {Project} - {Free} {Software} {Foundation}},
	url = {https://www.gnu.org/software/bash/},
	urldate = {2021-08-29},
	file = {Bash - GNU Project - Free Software Foundation:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/3IJGUQQI/bash.html:text/html},
}

@misc{noauthor_ubuntu_nodate,
	title = {Ubuntu 18.04.5 {LTS} ({Bionic} {Beaver})},
	url = {https://releases.ubuntu.com/18.04.5/},
	urldate = {2021-08-29},
	file = {Ubuntu 18.04.5 LTS (Bionic Beaver):/home/leonamtv/snap/zotero-snap/common/Zotero/storage/PN6FIW2A/18.04.5.html:text/html},
}

@misc{noauthor_iterate_nodate,
	title = {Iterate faster, innovate together {\textbar} {GitLab}},
	url = {https://about.gitlab.com/},
	urldate = {2021-08-29},
}

@misc{noauthor_nginx_nodate,
	title = {{NGINX} {\textbar} {High} {Performance} {Load} {Balancer}, {Web} {Server}, \& {Reverse} {Proxy}},
	url = {https://www.nginx.com/},
	urldate = {2021-08-29},
}

@misc{noauthor_sqlite_nodate,
	title = {{SQLite} {Home} {Page}},
	url = {https://www.sqlite.org/index.html},
	urldate = {2021-08-29},
	file = {SQLite Home Page:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/7FRRBG6U/index.html:text/html},
}

@misc{noauthor_download_nodate,
	title = {Download {Python} {\textbar} {Python}.org},
	url = {https://www.python.org/downloads/},
	urldate = {2021-08-29},
	file = {Download Python | Python.org:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/AS4S7BEJ/downloads.html:text/html},
}

@misc{noauthor_mysql_nodate,
	title = {{MySQL}},
	url = {https://www.mysql.com/},
	urldate = {2021-08-29},
	file = {MySQL:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/V6GG6FJQ/www.mysql.com.html:text/html},
}

@misc{noauthor_php_nodate,
	title = {{PHP}: {Hypertext} {Preprocessor}},
	url = {https://www.php.net/},
	urldate = {2021-08-29},
	file = {PHP\: Hypertext Preprocessor:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/WWHB66CH/www.php.net.html:text/html},
}

@misc{nodejs_nodejs_nodate,
	title = {Node.js},
	url = {https://nodejs.org/en/},
	abstract = {Node.js® is a JavaScript runtime built on Chrome's V8 JavaScript engine.},
	language = {en},
	urldate = {2021-08-29},
	journal = {Node.js},
	author = {Node.js},
}

@misc{noauthor_angular_nodate,
	title = {Angular},
	url = {https://angular.io/},
	urldate = {2021-08-29},
}

@misc{team_angular_nodate,
	title = {Angular {Material}},
	url = {https://material.angular.io/},
	abstract = {UI component infrastructure and Material Design components for Angular web applications.},
	language = {en-US},
	urldate = {2021-08-29},
	journal = {Angular Material},
	author = {Team, Angular Components},
}

@misc{noauthor_rudder_nodate,
	title = {{RUDDER} - {Continuous} {Auditing} \& {Configuration}},
	url = {https://www.rudder.io/},
	abstract = {WEBINAR, July 11th  – CVE: speed up vulnerability remediation to secure your systems (in French) Register Discover RUDDER Continuous Auditing \& Configuration RUDDER is a European, open source and multi-platform solution allowing you to manage configurations and compliance of your systems. Based on Continuous Configuration, we combine configuration management and continuous audit. Learn more Download … RUDDER – Continuous Auditing \& Configuration Read More »},
	language = {en-US},
	urldate = {2021-08-29},
	journal = {RUDDER},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/NAFTFE3T/www.rudder.io.html:text/html},
}

@misc{noauthor_production-grade_nodate,
	title = {Production-{Grade} {Container} {Orchestration}},
	url = {https://kubernetes.io/},
	abstract = {Production-Grade Container Orchestration},
	language = {en},
	urldate = {2021-08-29},
	journal = {Kubernetes},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/3BS6TK3N/kubernetes.io.html:text/html},
}

@misc{noauthor_swarm_2021,
	title = {Swarm mode overview},
	url = {https://docs.docker.com/engine/swarm/},
	abstract = {Docker Engine swarm mode overview},
	language = {en},
	urldate = {2021-08-29},
	journal = {Docker Documentation},
	month = aug,
	year = {2021},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/V8BJ6PRV/swarm.html:text/html},
}

@misc{gulli_deep_2019,
	title = {Deep {Learning} with {TensorFlow} 2 and {Keras} - {Google} {Books}},
	url = {https://www.google.com.br/books/edition/Deep_Learning_with_TensorFlow_2_and_Kera/hebZzAEACAAJ?hl=en},
	abstract = {Deep Learning with TensorFlow 2 and Keras, Second Edition teaches neural networks and deep learning techniques alongside TensorFlow (TF) and Keras. You'll learn how to write deep learning applications in the most powerful, popular, and scalable machine learning stack available.},
	urldate = {2021-08-22},
	author = {Gulli, Antonio and Kapoor, Amita and Pal, Sujit},
	month = dec,
	year = {2019},
	file = {Deep Learning with TensorFlow 2 and Keras - Google Books:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/2HAN97P4/hebZzAEACAAJ.html:text/html},
}

@misc{moreira_geracao_2019,
	title = {{GERAÇÃO} {DE} {IMAGENS} {EM} {SUPER} {RESOLUÇÃO} {COM} {REDES} {GERADORAS} {ADVERSÁRIAS}},
	url = {https://repositorio.ufsc.br/handle/123456789/202494},
	abstract = {A geração de novas amostras é um desafio atual da inteligência artifical.
Os modelos geradores tentam há muito tempo resolver esse problema,
porém, a complexidade enfrentada era tamanha que poucos modelos
foram desenvolvidos. Com o aumento do poder computacional e das
técnicas de aprendizagem profunda, essa área floresceu novamente. As
Redes Geradoras Adversárias (RAGs) representam o estado da arte dos
modelos geradores, fornecendo amostras de alta qualidade. Entretanto,
a qualidade dos materiais existentes afeta o pleno entendimento dessa
técnica. Neste trabalho, são vistos os principais aspectos teóricos e prá-
ticos desses modelos, com a realização de experimentos para suportar
e validar a teoria. Por fim, uma RAG estado da arte é utilizada para
resolver um problema atual: a super resolução de imagens, área que
aprimora a resolução de um sistema de imagens.},
	language = {pt},
	urldate = {2021-08-22},
	author = {Moreira, Fábio},
	month = jul,
	year = {2019},
}

@misc{wikimedia_multilayerneuralnetwork_2021,
	title = {{MultiLayerNeuralNetwork}},
	url = {https://upload.wikimedia.org/wikipedia/commons/4/47/MultiLayerNeuralNetwork_english.png},
	urldate = {2021-08-22},
	author = {Wikimedia},
	month = aug,
	year = {2021},
	file = {MultiLayerNeuralNetwork_english.png (620×347):/home/leonamtv/snap/zotero-snap/common/Zotero/storage/9VKQMYTU/MultiLayerNeuralNetwork_english.html:text/html},
}

@inproceedings{c_han_gan-based_2018,
	title = {{GAN}-based synthetic brain {MR} image generation},
	isbn = {1945-8452},
	doi = {10.1109/ISBI.2018.8363678},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	author = {{C. Han} and {H. Hayashi} and {L. Rundo} and {R. Araki} and {W. Shimoda} and {S. Muramatsu} and {Y. Furukawa} and {G. Mauri} and {H. Nakayama}},
	month = apr,
	year = {2018},
	note = {Journal Abbreviation: 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)},
	pages = {734--738},
}

@misc{noauthor_gan-based_nodate,
	title = {{GAN}-based synthetic brain {MR} image generation {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/8363678},
	urldate = {2021-08-22},
}

@inproceedings{xu_diversity-promoting_2018,
	address = {Brussels, Belgium},
	title = {Diversity-{Promoting} {GAN}: {A} {Cross}-{Entropy} {Based} {Generative} {Adversarial} {Network} for {Diversified} {Text} {Generation}},
	shorttitle = {Diversity-{Promoting} {GAN}},
	url = {https://aclanthology.org/D18-1428},
	doi = {10.18653/v1/D18-1428},
	abstract = {Existing text generation methods tend to produce repeated and ”boring” expressions. To tackle this problem, we propose a new text generation model, called Diversity-Promoting Generative Adversarial Network (DP-GAN). The proposed model assigns low reward for repeatedly generated text and high reward for ”novel” and fluent text, encouraging the generator to produce diverse and informative text. Moreover, we propose a novel language-model based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines.},
	urldate = {2021-08-22},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Jingjing and Ren, Xuancheng and Lin, Junyang and Sun, Xu},
	month = oct,
	year = {2018},
	pages = {3940--3949},
	file = {Full Text PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/6HIX2QC6/Xu et al. - 2018 - Diversity-Promoting GAN A Cross-Entropy Based Gen.pdf:application/pdf},
}

@article{oza_progressive_2019,
	title = {Progressive {Generative} {Adversarial} {Binary} {Networks} for {Music} {Generation}},
	url = {http://arxiv.org/abs/1903.04722},
	abstract = {Recent improvements in generative adversarial network (GAN) training techniques prove that progressively training a GAN drastically stabilizes the training and improves the quality of outputs produced. Adding layers after the previous ones have converged has proven to help in better overall convergence and stability of the model as well as reducing the training time by a sufficient amount. Thus we use this training technique to train the model progressively in the time and pitch domain i.e. starting from a very small time value and pitch range we gradually expand the matrix sizes until the end result is a completely trained model giving outputs having tensor sizes [4 (bar) x 96 (time steps) x 84 (pitch values) x 8 (tracks)]. As proven in previously proposed models deterministic binary neurons also help in improving the results. Thus we make use of a layer of deterministic binary neurons at the end of the generator to get binary valued outputs instead of fractional values existing between 0 and 1.},
	urldate = {2021-08-22},
	journal = {arXiv:1903.04722 [cs, eess]},
	author = {Oza, Manan and Vaghela, Himanshu and Srivastava, Kriti},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.04722},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/FPYXFN6T/Oza et al. - 2019 - Progressive Generative Adversarial Binary Networks.pdf:application/pdf;arXiv.org Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/3TFH5G3U/1903.html:text/html},
}

@misc{tsang_review_2020,
	title = {Review: {SRGAN} \& {SRResNet} — {Photo}-{Realistic} {Super} {Resolution} ({GAN} \& {Super} {Resolution})},
	shorttitle = {Review},
	url = {https://sh-tsang.medium.com/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490},
	abstract = {Using Generative Adversarial Network (GAN), Lower PSNR But with More Photo-Realistic Super-Resolved Image Can Be Obtained},
	language = {en},
	urldate = {2021-08-22},
	journal = {Medium},
	author = {Tsang, Sik-Ho},
	month = apr,
	year = {2020},
}

@misc{weisstein_convolution_2003,
	type = {Text},
	title = {Convolution},
	copyright = {Copyright 1999-2021 Wolfram Research, Inc.  See https://mathworld.wolfram.com/about/terms.html for a full terms of use statement.},
	url = {https://mathworld.wolfram.com/Convolution.html},
	abstract = {A convolution is an integral that expresses the amount of overlap of one function g as it is shifted over another function f. It therefore "blends" one function with another. For example, in synthesis imaging, the measured dirty map is a convolution of the "true" CLEAN map with the dirty beam (the Fourier transform of the sampling distribution). The convolution is sometimes also known by its German name, faltung ("folding"). Convolution is implemented in the...},
	language = {en},
	urldate = {2021-08-22},
	author = {Weisstein, Eric W.},
	month = jan,
	year = {2003},
	note = {Publisher: Wolfram Research, Inc.},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/CEVPZJFG/Convolution.html:text/html},
}

@misc{wikipedia_convolucao_2020,
	title = {Convolução},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://pt.wikipedia.org/w/index.php?title=Convolu%C3%A7%C3%A3o&oldid=59483262},
	abstract = {Em matemática, particularmente na área de análise funcional e processamento do sinal, convolução é um operador linear que, a partir de duas funções dadas, resulta numa terceira que mede a soma do produto dessas funções ao longo da região subentendida pela superposição delas em função do deslocamento existente entre elas. O conceito de convolução está ligado à integral de superposição na Óptica de Fourier, à integral de Duhamel na teoria das vibrações, ao Teorema de Borel no estudo de sistemas lineares invariantes no tempo, ao conceito de média móvel, às funções de correlação e de autocorrelação em estatística e em processamento de sinais, e a diversos conceitos usados em análise de imagens, como digitalização, alisamento, embaçamento e aberração cromática.},
	language = {pt},
	urldate = {2021-08-22},
	journal = {Wikipédia, a enciclopédia livre},
	author = {Wikipedia},
	month = sep,
	year = {2020},
	note = {Page Version ID: 59483262},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/7RLSY3FF/index.html:text/html},
}

@misc{kumar_semantic_2020,
	title = {Semantic {Image} {Segmentation} using {Fully} {Convolutional} {Networks}},
	url = {https://towardsdatascience.com/semantic-image-segmentation-using-fully-convolutional-networks-bf0189fa3eb8},
	abstract = {Severstal Steel Defect Detection — a case study},
	language = {en},
	urldate = {2021-08-22},
	journal = {Medium},
	author = {Kumar, Arun},
	month = may,
	year = {2020},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/JNEHMQWW/semantic-image-segmentation-using-fully-convolutional-networks-bf0189fa3eb8.html:text/html},
}

@misc{brownlee_gentle_nodate,
	title = {A {Gentle} {Introduction} to {Pooling} {Layers} for {Convolutional} {Neural} {Networks}},
	url = {https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/},
	urldate = {2021-08-22},
	author = {Brownlee, Jason},
	file = {A Gentle Introduction to Pooling Layers for Convolutional Neural Networks:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/4CNWM4L8/pooling-layers-for-convolutional-neural-networks.html:text/html},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	number = {4},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	year = {1943},
	pages = {115--133},
}

@misc{chrislb_english_2005,
	title = {English:  {Diagram} of an artificial neuron.},
	copyright = {Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled GNU Free Documentation License.http://www.gnu.org/copyleft/fdl.htmlGFDLGNU Free Documentation Licensetruetrue},
	shorttitle = {English},
	url = {https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png},
	urldate = {2021-08-22},
	author = {{Chrislb}},
	month = jul,
	year = {2005},
	file = {Wikimedia Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/LCL5MG9N/FileArtificialNeuronModel_english.html:text/html},
}

@article{zhang_dive_2021,
	title = {Dive into {Deep} {Learning}},
	url = {http://arxiv.org/abs/2106.11342},
	abstract = {This open-source book represents our attempt to make deep learning approachable, teaching readers the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code. Our goal is to offer a resource that could (i) be freely available for everyone; (ii) offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist; (iii) include runnable code, showing readers how to solve problems in practice; (iv) allow for rapid updates, both by us and also by the community at large; (v) be complemented by a forum for interactive discussion of technical details and to answer questions.},
	urldate = {2021-08-22},
	journal = {arXiv:2106.11342 [cs]},
	author = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
	month = jul,
	year = {2021},
	note = {arXiv: 2106.11342},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: (HTML) https://D2L.ai (GitHub) https://github.com/d2l-ai/d2l-en/},
	file = {arXiv Fulltext PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/KBXDN2I9/Zhang et al. - 2021 - Dive into Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/HXEE8TB3/2106.html:text/html},
}

@misc{chrislb_english_2005-1,
	title = {English:  {Diagram} of an artificial neuron.},
	copyright = {Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled GNU Free Documentation License.http://www.gnu.org/copyleft/fdl.htmlGFDLGNU Free Documentation Licensetruetrue},
	shorttitle = {English},
	url = {https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png},
	urldate = {2021-08-22},
	author = {{Chrislb}},
	month = jul,
	year = {2005},
	file = {Wikimedia Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/MLSNAQSX/FileArtificialNeuronModel_english.html:text/html},
}

@book{haykin_neural_2009,
	title = {Neural {Networks} and {Learning} {Machines}},
	isbn = {978-0-13-147139-9},
	abstract = {Fluid and authoritative, this well-organized book represents the first comprehensive treatment of neural networks and learning machines from an engineering perspective, providing extensive, state-of-the-art coverage that will expose readers to the myriad facets of neural networks and help them appreciate the technology's origin, capabilities, and potential applications. KEY TOPICS: Examines all the important aspects of this emerging technology, covering the learning process, back propogation, radial basis functions, recurrent networks, self-organizing systems, modular networks, temporal processing, neurodynamics, and VLSI implementation. Integrates computer experiments throughout to demonstrate how neural networks are designed and perform in practice. Chapter objectives, problems, worked examples, a bibliography, photographs, illustrations, and a thorough glossary all reinforce concepts throughout. New chapters delve into such areas as support vector machines, and reinforcement learning/neurodynamic programming, Rosenblatt's Perceptron, Least-Mean-Square Algorithm, Regularization Theory, Kernel Methods and Radial-Basis function networks (RBF), and Bayseian Filtering for State Estimation of Dynamic Systems. An entire chapter of case studies illustrates the real-life, practical applications of neural networks. A highly detailed bibliography is included for easy reference. MARKET: For professional engineers and research scientists.   Matlab codes used for the computer experiments in the text are available for download at: http: //www.pearsonhighered.com/haykin/},
	language = {en},
	publisher = {Prentice Hall},
	author = {Haykin, Simon S.},
	year = {2009},
	note = {Google-Books-ID: K7P36lKzI\_QC},
	keywords = {Computers / Data Science / Neural Networks},
}

@article{karras_progressive_2018,
	title = {{PROGRESSIVE} {GROWING} {OF} {GANS} {FOR} {IMPROVED} {QUALITY}, {STABILITY}, {AND} {VARIATION}},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly ﬁne details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 10242. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.},
	language = {en},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	year = {2018},
	pages = {26},
	file = {Karras et al. - 2018 - PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, .pdf:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/4CA2M6LA/Karras et al. - 2018 - PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, .pdf:application/pdf},
}

@article{ledig_photo-realistic_2016,
	title = {Photo-{Realistic} {Single} {Image} {Super}-{Resolution} {Using} a {Generative} {Adversarial} {Network}},
	url = {https://arxiv.org/abs/1609.04802v5},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	language = {en},
	urldate = {2021-07-27},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	month = sep,
	year = {2016},
	file = {Full Text PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/NCYS6BWL/Ledig et al. - 2016 - Photo-Realistic Single Image Super-Resolution Usin.pdf:application/pdf;Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/B9P3VWP9/1609.html:text/html},
}

@misc{park_implementing_2021,
	title = {Implementing {SRResnet}/{SRGAN} {Super} {Resolution} with {Tensorflow}},
	url = {https://medium.com/analytics-vidhya/implementing-srresnet-srgan-super-resolution-with-tensorflow-89900d2ec9b2},
	abstract = {Implementation of the paper “Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network”.},
	language = {en},
	urldate = {2021-07-27},
	journal = {Analytics Vidhya},
	author = {Park, Sieun},
	month = mar,
	year = {2021},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/LQYQVXRQ/implementing-srresnet-srgan-super-resolution-with-tensorflow-89900d2ec9b2.html:text/html},
}

@misc{zuppichini_deploy_2018,
	title = {Deploy {TensorFlow} models},
	url = {https://towardsdatascience.com/deploy-tensorflow-models-9813b5a705d5},
	abstract = {Learn how to deploy your model to production},
	language = {en},
	urldate = {2021-07-27},
	journal = {Medium},
	author = {Zuppichini, Francesco},
	month = jun,
	year = {2018},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/ZX3FVSM9/deploy-tensorflow-models-9813b5a705d5.html:text/html},
}

@article{karras_progressive_2017,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {https://arxiv.org/abs/1710.10196v3},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
	language = {en},
	urldate = {2021-07-27},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	month = oct,
	year = {2017},
	file = {Full Text PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/UIBKDU5G/Karras et al. - 2017 - Progressive Growing of GANs for Improved Quality, .pdf:application/pdf;Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/377STPVT/1710.html:text/html},
}

@misc{noauthor_generative_2021,
	title = {Generative adversarial network},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Generative_adversarial_network&oldid=1035451643},
	abstract = {A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent's gain is another agent's loss).
Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proven useful for semi-supervised learning, fully supervised learning, and reinforcement learning.The core idea of a GAN is based on the "indirect" training through the discriminator, which itself is also being updated dynamically. This basically means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.},
	language = {en},
	urldate = {2021-07-27},
	journal = {Wikipedia},
	month = jul,
	year = {2021},
	note = {Page Version ID: 1035451643},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/RTJ9M7MF/index.html:text/html},
}

@article{gupta_super-resolution_2020,
	series = {International {Conference} on {Smart} {Sustainable} {Intelligent} {Computing} and {Applications} under {ICITETM2020}},
	title = {Super-{Resolution} using {GANs} for {Medical} {Imaging}},
	volume = {173},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050920315076},
	doi = {10.1016/j.procs.2020.06.005},
	abstract = {Generative Adversarial Models (GANs) have been quite popular and are currently and active area of research. They can be used for generative new data and study adversarial samples and attacks. We have used the similar approach to apply super-resolution to medical images. In Radiology MRI is a commonly used method to produce medical imaging but the limitations of lab equipment and health hazard of being in an MRI radiation environment to obtain good quality scans lead to lower quality scans and also it takes a lot of time to get a high-resolution data. This problem can be solved by using super-resolution using deep learning as a post-processing step to improve the resolution of the scans. Super-resolution is a process of generating higher resolution images from lower resolution data. For this, we are proposing a generative adversarial network architecture which is a dual neural network designed to generate lifelike images. In this deep learning algorithm, two neural networks compete with each other to improve alternatively. Given a training set, this technique learns to generate new data with the same statistics as the training set. To apply this technique to our problem statement we are using generator as the network to improve the resolution and discriminator as a network to train generator better. We used transfer learning in our generative neural network and training our discriminator from scratch and using the perceptual loss [1] to train our network. This will help in improving the performance of the network. We are using Lung MRI scans of tuberculosis with a set of 216 MRI samples containing around 60-130 channels each and each channel having 512x512 dimensions.},
	language = {en},
	urldate = {2021-07-22},
	journal = {Procedia Computer Science},
	author = {Gupta, Rohit and Sharma, Anurag and Kumar, Anupam},
	month = jan,
	year = {2020},
	keywords = {Artificial Intelligence, Computer Vision, Deep Learning, Generative Adversarial Network, Magnetic resonance imaging, Super resolution, Transfer Learning},
	pages = {28--35},
	file = {ScienceDirect Full Text PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/57MKA2EF/Gupta et al. - 2020 - Super-Resolution using GANs for Medical Imaging.pdf:application/pdf;ScienceDirect Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/63RQXEVX/S1877050920315076.html:text/html},
}

@misc{noauthor_notitle_nodate,
}

@misc{noauthor_notitle_nodate-1,
}

@article{sun_super_2019,
	title = {Super {Resolution} {Reconstruction} of {Images} {Based} on {Interpolation} and {Full} {Convolutional} {Neural} {Network} and {Application} in {Medical} {Fields}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2960828},
	abstract = {The traditional image to enlarge algorithms include nearest neighbor interpolation, bilinear interpolation and high-order interpolation. In order to achieve super-resolution reconstruction of images, a new algorithm combining traditional algorithms and deep learning is proposed. The framework is divided into two parts. Firstly, the deep reconstruction of the low-resolution data is performed by the ability of deep learning to extract features automatically. Then, combining with the traditional interpolation reconstruction results, the deep learning algorithm is used again for training and learning, and finally the high-resolution reconstructed data is obtained. The algorithm is validated using an online public test data set. The results show that the algorithm has a significant effect on the MSE (mean squared error) and PSNR (Peak Signal to Noise Ratio). Compared with the traditional interpolation algorithm and the single deep learning algorithm, the proposed algorithm has higher performance. Moreover, the proposed algorithm is perfect for the reconstruction of the details, the outline is clear, and the high-quality image is obtained.},
	journal = {IEEE Access},
	author = {Sun, Na and Li, Huina},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Biomedical imaging, deep learning, Deep learning, Image edge detection, Image reconstruction, Image resolution, Interpolation, MSE, PSNR, reconstruction, super-resolution},
	pages = {186470--186479},
	file = {IEEE Xplore Abstract Record:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/ESVTVJU3/8939373.html:text/html;IEEE Xplore Full Text PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/WC7Z4SRX/Sun e Li - 2019 - Super Resolution Reconstruction of Images Based on.pdf:application/pdf},
}

@inproceedings{khaledyan_low-cost_2020,
	address = {Seattle, WA, USA},
	title = {Low-{Cost} {Implementation} of {Bilinear} and {Bicubic} {Image} {Interpolation} for {Real}-{Time} {Image} {Super}-{Resolution}},
	isbn = {978-1-72817-388-7},
	url = {https://ieeexplore.ieee.org/document/9342625/},
	doi = {10.1109/GHTC46280.2020.9342625},
	abstract = {Super-resolution imaging (S.R.) is a series of techniques that enhance the resolution of an imaging system, especially in surveillance cameras where simplicity and low cost are of great importance. S.R. image reconstruction can be viewed as a three-stage process: image interpolation, image registration, and fusion. Image interpolation is one of the most critical steps in the S.R. algorithms and has a significant influence on the quality of the output image. In this paper, two hardware-efﬁcient interpolation methods are proposed for these platforms, mainly for the mobile application. Experiments and results on the synthetic and real image sequences clearly validate the performance of the proposed scheme. They indicate that the proposed approach is practically applicable to real-world applications. The algorithms are implemented in a Field Programmable Gate Array (FPGA) device using a pipelined architecture. The implementation results show the advantages of the proposed methods regarding area, performance, and output quality.},
	language = {en},
	urldate = {2021-07-22},
	booktitle = {2020 {IEEE} {Global} {Humanitarian} {Technology} {Conference} ({GHTC})},
	publisher = {IEEE},
	author = {Khaledyan, Donya and Amirany, Abdolah and Jafari, Kian and Moaiyeri, Mohammad Hossein and Khuzani, Abolfazl Zargari and Mashhadi, Najmeh},
	month = oct,
	year = {2020},
	pages = {1--5},
	file = {Khaledyan et al. - 2020 - Low-Cost Implementation of Bilinear and Bicubic Im.pdf:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/CH6GZNX5/Khaledyan et al. - 2020 - Low-Cost Implementation of Bilinear and Bicubic Im.pdf:application/pdf},
}

@inproceedings{gavade_super_2013,
	title = {{SUPER} {RESOLUTION} {IMAGE} {RECONSTRUCTION} {BY} {USING} {BICUBIC} {INTERPOLATION}},
	doi = {10.13140/RG.2.1.4909.0401},
	abstract = {Interpolation is the process of estimating unknown values from known sample values
and estimating continuous samples from discrete samples. Image processing applications of interpolation
include image magnification or reduction, sub-pixel image registration, to correct spatial distortions,
image decompression, image super resolution reconstruction. There are many interpolation techniques,
few famous of them include nearest neighbor interpolation, bilinear and cubic spline interpolation. Since
Interpolation does not give good results within an image processing environment, and image data is
generally acquired at a much lower sampling rate, the mapping between the unknown high-resolution
image and the low-resolution image is not invertible, and thus a unique solution to the inverse problem
cannot be computed. In this paper, we review one of the most useful interpolation method, i.e. the
bicubic interpolation. We first give briefing on earlier interpolation methods and then explain the bicubic
interpolation method and why it is widely used in the image processing applications. Finally we put our
main emphasis on its implementation in MATLAB. We also compare this method with various other
techniques and spot out some limitations.

Keywords: Interpolation, Spatial Distortions, Decompression, Super-Resolution.},
	author = {Gavade, Anil and Sane, Prasana},
	month = oct,
	year = {2013},
	file = {Full Text PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/CQ63V8KG/Gavade e Sane - 2013 - SUPER RESOLUTION IMAGE RECONSTRUCTION BY USING BIC.pdf:application/pdf},
}

@incollection{fleet_single-image_2014,
	address = {Cham},
	title = {Single-{Image} {Super}-{Resolution}: {A} {Benchmark}},
	volume = {8692},
	isbn = {978-3-319-10592-5 978-3-319-10593-2},
	shorttitle = {Single-{Image} {Super}-{Resolution}},
	url = {http://link.springer.com/10.1007/978-3-319-10593-2_25},
	abstract = {Single-image super-resolution is of great importance for vision applications, and numerous algorithms have been proposed in recent years. Despite the demonstrated success, these results are often generated based on diﬀerent assumptions using diﬀerent datasets and metrics. In this paper, we present a systematic benchmark evaluation for state-of-the-art single-image super-resolution algorithms. In addition to quantitative evaluations based on conventional full-reference metrics, human subject studies are carried out to evaluate image quality based on visual perception. The benchmark evaluations demonstrate the performance and limitations of state-of-the-art algorithms which sheds light on future research in single-image super-resolution.},
	language = {en},
	urldate = {2021-07-22},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Yang, Chih-Yuan and Ma, Chao and Yang, Ming-Hsuan},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	doi = {10.1007/978-3-319-10593-2_25},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {372--386},
	file = {Yang et al. - 2014 - Single-Image Super-Resolution A Benchmark.pdf:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/B7DEKJJT/Yang et al. - 2014 - Single-Image Super-Resolution A Benchmark.pdf:application/pdf},
}

@article{yang_deep_2019,
	title = {Deep {Learning} for {Single} {Image} {Super}-{Resolution}: {A} {Brief} {Review}},
	volume = {21},
	issn = {1520-9210, 1941-0077},
	shorttitle = {Deep {Learning} for {Single} {Image} {Super}-{Resolution}},
	url = {http://arxiv.org/abs/1808.03344},
	doi = {10.1109/TMM.2019.2919431},
	abstract = {Single image super-resolution (SISR) is a notoriously challenging ill-posed problem, which aims to obtain a high-resolution (HR) output from one of its low-resolution (LR) versions. To solve the SISR problem, recently powerful deep learning algorithms have been employed and achieved the state-of-the-art performance. In this survey, we review representative deep learning-based SISR methods, and group them into two categories according to their major contributions to two essential aspects of SISR: the exploration of efficient neural network architectures for SISR, and the development of effective optimization objectives for deep SISR learning. For each category, a baseline is firstly established and several critical limitations of the baseline are summarized. Then representative works on overcoming these limitations are presented based on their original contents as well as our critical understandings and analyses, and relevant comparisons are conducted from a variety of perspectives. Finally we conclude this review with some vital current challenges and future trends in SISR leveraging deep learning algorithms.},
	number = {12},
	urldate = {2021-07-22},
	journal = {IEEE Transactions on Multimedia},
	author = {Yang, Wenming and Zhang, Xuechen and Tian, Yapeng and Wang, Wei and Xue, Jing-Hao},
	month = dec,
	year = {2019},
	note = {arXiv: 1808.03344},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {3106--3121},
	annote = {Comment: Accepted by IEEE Transactions on Multimedia (TMM)},
	file = {arXiv Fulltext PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/N67VYISW/Yang et al. - 2019 - Deep Learning for Single Image Super-Resolution A.pdf:application/pdf;arXiv.org Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/WTBUHDEG/1808.html:text/html},
}

@article{pathak_context_2016,
	title = {Context {Encoders}: {Feature} {Learning} by {Inpainting}},
	shorttitle = {Context {Encoders}},
	url = {http://arxiv.org/abs/1604.07379},
	abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
	urldate = {2021-07-22},
	journal = {arXiv:1604.07379 [cs]},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	month = nov,
	year = {2016},
	note = {arXiv: 1604.07379},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Graphics},
	annote = {Comment: New results on ImageNet Generation},
	file = {arXiv Fulltext PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/6RQEZB5Z/Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:application/pdf;arXiv.org Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/VWQQWU83/1604.html:text/html},
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2021-07-22},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2661},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/DXARDUMD/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/58QW2RW6/1406.html:text/html},
}

@article{goodfellow_nips_2017,
	title = {{NIPS} 2016 {Tutorial}: {Generative} {Adversarial} {Networks}},
	shorttitle = {{NIPS} 2016 {Tutorial}},
	url = {http://arxiv.org/abs/1701.00160},
	abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	urldate = {2021-07-22},
	journal = {arXiv:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	month = apr,
	year = {2017},
	note = {arXiv: 1701.00160},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: v2-v4 are all typo fixes. No substantive changes relative to v1},
	file = {arXiv Fulltext PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/6VGXBNEW/Goodfellow - 2017 - NIPS 2016 Tutorial Generative Adversarial Network.pdf:application/pdf;arXiv.org Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/288K6P3Z/1701.html:text/html},
}

@article{ledig_photo-realistic_2017,
	title = {Photo-{Realistic} {Single} {Image} {Super}-{Resolution} {Using} a {Generative} {Adversarial} {Network}},
	url = {http://arxiv.org/abs/1609.04802},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	urldate = {2021-07-22},
	journal = {arXiv:1609.04802 [cs, stat]},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	month = may,
	year = {2017},
	note = {arXiv: 1609.04802},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	annote = {Comment: 19 pages, 15 figures, 2 tables, accepted for oral presentation at CVPR, main paper + some supplementary material},
	file = {arXiv Fulltext PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/5GMKCKGR/Ledig et al. - 2017 - Photo-Realistic Single Image Super-Resolution Usin.pdf:application/pdf;arXiv.org Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/W6TF3SB9/1609.html:text/html},
}

@article{takemura_algoritmos_nodate,
	title = {Algoritmos para {Super}-{Resolução} de {Imagens} {Baseados} nas {Filtragens} de {Wiener} e {Adaptativa} {Usando} a {Transformada} {Wavelet}},
	language = {pt},
	author = {Takemura, Erica Sayuri},
	url = {http://www.pee.ufrj.br/index.php/pt/producao-academica/dissertacoes-de-mestrado/2013-1/2013121601-2013121601/file},
	year = {2013},
	pages = {82},
	file = {Takemura - Algoritmos para Super-Resolução de Imagens Baseado.pdf:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/GDI8EYP7/Takemura - Algoritmos para Super-Resolução de Imagens Baseado.pdf:application/pdf},
}

@article{takemura_algoritmos_nodate-1,
	title = {Algoritmos para {Super}-{Resolução} de {Imagens} {Baseados} nas {Filtragens} de {Wiener} e {Adaptativa} {Usando} a {Transformada} {Wavelet}},
	language = {pt},
	author = {Takemura, Erica Sayuri},
	url = {http://www.pee.ufrj.br/index.php/pt/producao-academica/dissertacoes-de-mestrado/2013-1/2013121601-2013121601/file2},
	pages = {82},
	year = {2013},
	file = {Takemura - Algoritmos para Super-Resolução de Imagens Baseado.pdf:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/GVYINDY9/Takemura - Algoritmos para Super-Resolução de Imagens Baseado.pdf:application/pdf},
}

@article{zhu_gan-based_2020,
	title = {{GAN}-{Based} {Image} {Super}-{Resolution} with a {Novel} {Quality} {Loss}},
	volume = {2020},
	issn = {1024-123X},
	url = {https://www.hindawi.com/journals/mpe/2020/5217429/},
	doi = {10.1155/2020/5217429},
	abstract = {Single image super-resolution (SISR) has been a very attractive research topic in recent years. Breakthroughs in SISR have been achieved due to deep learning and generative adversarial networks (GANs). However, the generated image still suffers from undesired artifacts. In this paper, we propose a new method named GMGAN for SISR tasks. In this method, to generate images more in line with human vision system (HVS), we design a quality loss by integrating an image quality assessment (IQA) metric named gradient magnitude similarity deviation (GMSD). To our knowledge, it is the first time to truly integrate an IQA metric into SISR. Moreover, to overcome the instability of the original GAN, we use a variant of GANs named improved training of Wasserstein GANs (WGAN-GP). Besides GMGAN, we highlight the importance of training datasets. Experiments show that GMGAN with quality loss and WGAN-GP can generate visually appealing results and set a new state of the art. In addition, large quantity of high-quality training images with rich textures can benefit the results.},
	language = {en},
	urldate = {2021-07-21},
	journal = {Mathematical Problems in Engineering},
	author = {Zhu, Xining and Zhang, Lin and Zhang, Lijun and Liu, Xiao and Shen, Ying and Zhao, Shengjie},
	month = feb,
	year = {2020},
	note = {Publisher: Hindawi},
	pages = {e5217429},
	file = {Full Text PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/JXRE9NI6/Zhu et al. - 2020 - GAN-Based Image Super-Resolution with a Novel Qual.pdf:application/pdf;Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/C34UNJT8/5217429.html:text/html},
}

@misc{vish_how_2020,
	title = {How {Much} {Data} {Is} {Created} {Every} {Day} in 2021? [{You}'ll be shocked!]},
	shorttitle = {How {Much} {Data} {Is} {Created} {Every} {Day} in 2021?},
	url = {https://techjury.net/blog/how-much-data-is-created-every-day/},
	abstract = {How much data is created every day? If "a lot" isn't enough for you, then check out our list of data growth statistics to find the answer!},
	language = {en-US},
	urldate = {2021-07-19},
	journal = {TechJury},
	author = {{Vish}},
	month = jun,
	year = {2020},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/JEBBN2B4/how-much-data-is-created-every-day.html:text/html},
}

@misc{noauthor_review_2020,
	title = {A {Review} of {Image} {Super}-{Resolution}},
	url = {https://blog.paperspace.com/image-super-resolution/},
	abstract = {Here we cover what image super-resolution is, its applications, the taxonomy of super-resolution algorithms, and their advantages and limitations.},
	language = {en},
	urldate = {2021-07-19},
	journal = {Paperspace Blog},
	month = sep,
	year = {2020},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/G7H9GQ9I/image-super-resolution.html:text/html},
}

@article{hagan_introduction_2002,
	title = {An introduction to the use of neural networks in control systems},
	volume = {12},
	copyright = {Copyright © 2002 John Wiley \& Sons, Ltd.},
	issn = {1099-1239},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rnc.727},
	doi = {https://doi.org/10.1002/rnc.727},
	abstract = {The purpose of this paper is to provide a quick overview of neural networks and to explain how they can be used in control systems. We introduce the multilayer perceptron neural network and describe how it can be used for function approximation. The backpropagation algorithm (including its variations) is the principal procedure for training multilayer perceptrons; it is briefly described here. Care must be taken, when training perceptron networks, to ensure that they do not overfit the training data and then fail to generalize well in new situations. Several techniques for improving generalization are discussed. The paper also presents three control architectures: model reference adaptive control, model predictive control, and feedback linearization control. These controllers demonstrate the variety of ways in which multilayer perceptron neural networks can be used as basic building blocks. We demonstrate the practical implementation of these controllers on three applications: a continuous stirred tank reactor, a robot arm, and a magnetic levitation system. Copyright © 2002 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {11},
	urldate = {2020-11-16},
	journal = {International Journal of Robust and Nonlinear Control},
	author = {Hagan, Martin T. and Demuth, Howard B. and Jesús, Orlando De},
	year = {2002},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rnc.727},
	keywords = {feedback linearization, model predictive control, model reference control, neurocontrol},
	pages = {959--985},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/K3I9NZGU/rnc.html:text/html;Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/VN373PDV/rnc.html:text/html;Versão submetida:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/FC22LQ4T/Hagan et al. - 2002 - An introduction to the use of neural networks in c.pdf:application/pdf},
}

@misc{openneuro_openneuro_2022,
	title = {{OpenNeuro}},
	url = {https://openneuro.org/},
	urldate = {2022-06-09},
	journal = {OpenNeuro},
	author = {{OpenNeuro}},
	month = jun,
	year = {2022},
	file = {OpenNeuro:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/CRF6VK99/openneuro.org.html:text/html},
}

@misc{openfmri_openfmri_2022,
	title = {{OpenfMRI}},
	url = {https://openfmri.org/},
	urldate = {2022-06-09},
	journal = {OpenfMRI},
	author = {{OpenfMRI}},
	month = jun,
	year = {2022},
	file = {OpenfMRI:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/WMNUXNCU/openfmri.org.html:text/html},
}

@misc{fastmri_fastmri_2022,
	title = {{FastMRI}},
	url = {https://fastmri.org/},
	abstract = {Accelerating MR Imaging with AI},
	language = {en},
	urldate = {2022-06-09},
	journal = {FastMRI},
	author = {{FastMRI}},
	month = jun,
	year = {2022},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/Y2SFNCHP/fastmri.org.html:text/html},
}

@misc{saha_dynamic_2021,
	title = {Dynamic contrast-enhanced magnetic resonance images of breast cancer patients with tumor locations},
	copyright = {Creative Commons Attribution Non Commercial 4.0 International},
	url = {https://wiki.cancerimagingarchive.net/x/15MvB},
	doi = {10.7937/TCIA.E3SV-RE93},
	abstract = {Breast MRI is a common image modality to assess extent of disease in breast cancer patients. Recent studies show that MRI has a potential in prognosis of patient sort and long term outcomes as well as predicting pathological and genomic features of the tumors. However, large, well annotated datasets are needed to make further progress in the field. We share such a dataset here.
In this dataset, we share MRI imaging and other data for 922 patients with invasive breast cancer. Their prone position axial breast MRI images were acquired by 1.5T or 3T scanners. Following MRI sequences are shared: a non-fat saturated T1-weighted sequence, a fat-saturated gradient echo T1-weighted pre-contrast sequence, and mostly three to four post-contrast sequences.
The images are associated with a variety of metadata. Specifically, we are sharing:
1.	Demographic, clinical, pathology, treatment, outcomes, and genomic data. This data has been collected from a variety of sources including clinical notes and has served as a source for multiple published papers on radiogenomics, outcomes prediction, and other.
2.	529 imaging features which represent a variety of imaging characteristics including size, shape, texture, and enhancement of both the tumor and the surrounding tissue, which is combined of features commonly published in the literature, as well as the features developed in our lab. These features were used in our previous paper (https://doi.org/10.1016/j.eswa.2017.06.029 )
3.	Annotation boxes provided by radiologists that indicate locations of the lesions in the images.},
	urldate = {2022-06-09},
	publisher = {The Cancer Imaging Archive},
	author = {Saha, Ashirbani and Harowicz, Michael R. and Grimm, Lars J. and Weng, Jingxi and Cain, E. H. and Kim, C. E. and Ghate, S. V. and Walsh, R. and Mazurowski, Maciej A.},
	year = {2021},
}

@article{saha_machine_2018,
	title = {A machine learning approach to radiogenomics of breast cancer: a study of 922 subjects and 529 {DCE}-{MRI} features},
	volume = {119},
	issn = {0007-0920, 1532-1827},
	shorttitle = {A machine learning approach to radiogenomics of breast cancer},
	url = {http://www.nature.com/articles/s41416-018-0185-8},
	doi = {10.1038/s41416-018-0185-8},
	language = {en},
	number = {4},
	urldate = {2022-06-09},
	journal = {British Journal of Cancer},
	author = {Saha, Ashirbani and Harowicz, Michael R. and Grimm, Lars J. and Kim, Connie E. and Ghate, Sujata V. and Walsh, Ruth and Mazurowski, Maciej A.},
	month = aug,
	year = {2018},
	pages = {508--516},
	file = {Full Text:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/IIKFAUKX/Saha et al. - 2018 - A machine learning approach to radiogenomics of br.pdf:application/pdf},
}

@article{clark_cancer_2013,
	title = {The {Cancer} {Imaging} {Archive} ({TCIA}): {Maintaining} and {Operating} a {Public} {Information} {Repository}},
	volume = {26},
	issn = {0897-1889, 1618-727X},
	shorttitle = {The {Cancer} {Imaging} {Archive} ({TCIA})},
	url = {http://link.springer.com/10.1007/s10278-013-9622-7},
	doi = {10.1007/s10278-013-9622-7},
	language = {en},
	number = {6},
	urldate = {2022-06-09},
	journal = {Journal of Digital Imaging},
	author = {Clark, Kenneth and Vendt, Bruce and Smith, Kirk and Freymann, John and Kirby, Justin and Koppel, Paul and Moore, Stephen and Phillips, Stanley and Maffitt, David and Pringle, Michael and Tarbox, Lawrence and Prior, Fred},
	month = dec,
	year = {2013},
	pages = {1045--1057},
	file = {Full Text:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/HEPBJ98D/Clark et al. - 2013 - The Cancer Imaging Archive (TCIA) Maintaining and.pdf:application/pdf},
}

@misc{saha_dynamic_2021-1,
	title = {Dynamic contrast-enhanced magnetic resonance images of breast cancer patients with tumor locations},
	copyright = {Creative Commons Attribution Non Commercial 4.0 International},
	url = {https://wiki.cancerimagingarchive.net/x/15MvB},
	doi = {10.7937/TCIA.E3SV-RE93},
	abstract = {Breast MRI is a common image modality to assess extent of disease in breast cancer patients. Recent studies show that MRI has a potential in prognosis of patient sort and long term outcomes as well as predicting pathological and genomic features of the tumors. However, large, well annotated datasets are needed to make further progress in the field. We share such a dataset here.
In this dataset, we share MRI imaging and other data for 922 patients with invasive breast cancer. Their prone position axial breast MRI images were acquired by 1.5T or 3T scanners. Following MRI sequences are shared: a non-fat saturated T1-weighted sequence, a fat-saturated gradient echo T1-weighted pre-contrast sequence, and mostly three to four post-contrast sequences.
The images are associated with a variety of metadata. Specifically, we are sharing:
1.	Demographic, clinical, pathology, treatment, outcomes, and genomic data. This data has been collected from a variety of sources including clinical notes and has served as a source for multiple published papers on radiogenomics, outcomes prediction, and other.
2.	529 imaging features which represent a variety of imaging characteristics including size, shape, texture, and enhancement of both the tumor and the surrounding tissue, which is combined of features commonly published in the literature, as well as the features developed in our lab. These features were used in our previous paper (https://doi.org/10.1016/j.eswa.2017.06.029 )
3.	Annotation boxes provided by radiologists that indicate locations of the lesions in the images.},
	urldate = {2022-06-09},
	publisher = {The Cancer Imaging Archive},
	author = {Saha, Ashirbani and Harowicz, Michael R. and Grimm, Lars J. and Weng, Jingxi and Cain, E. H. and Kim, C. E. and Ghate, S. V. and Walsh, R. and Mazurowski, Maciej A.},
	year = {2021},
}

@misc{noauthor_duke_nodate,
	title = {Duke {Breast} {Cancer} {MRI}},
	url = {https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=70226903#70226903bcab02c187174a288dbcbf95d26179e8},
}

@misc{pydicom_pydicom_2022,
	title = {Pydicom},
	url = {https://pydicom.github.io/},
	urldate = {2022-06-16},
	journal = {Pydicom},
	author = {{Pydicom}},
	month = jun,
	year = {2022},
	file = {Pydicom |:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/F6NL6PB8/pydicom.github.io.html:text/html},
}

@misc{n00mkrad_magickutils_2022,
	title = {{MagickUtils}},
	url = {https://github.com/n00mkrad/magick-utils},
	abstract = {Toolkit for image processing},
	urldate = {2022-06-19},
	author = {N00MKRAD},
	month = jun,
	year = {2022},
	note = {original-date: 2020-07-20T15:21:16Z},
}

@misc{nvidia_geforce_2022,
	title = {{GeForce} {920M} {Dedicated} {Graphics} for {Laptops}},
	url = {https://www.nvidia.com/en-us/geforce/gaming-laptops/geforce-920m/specifications/},
	abstract = {GeForce 920M accelerates graphics so you can enjoy movies, music, and gaming better and faster. Get great performance and longer battery life you need for work and play.},
	language = {en-us},
	urldate = {2022-06-20},
	journal = {GeForce 920M Dedicated Graphics for Laptops},
	author = {{NVIDIA}},
	month = jun,
	year = {2022},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/J6P8B4XE/specifications.html:text/html},
}

@misc{technical_city_nvidia_2022,
	title = {{NVIDIA} {GeForce} {920M}},
	url = {https://technical.city/en/video/GeForce-920M},
	abstract = {A thorough insight into technical specs and benchmarks of NVIDIA 920M.},
	language = {en},
	urldate = {2022-06-20},
	author = {{Technical City}},
	month = jun,
	year = {2022},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/ZG5JF963/GeForce-920M.html:text/html},
}

@misc{ryles_what_2022,
	title = {What are {CUDA} {Cores}?},
	url = {https://www.trustedreviews.com/explainer/what-are-cuda-cores-4226433},
	abstract = {On the lookout for a new GPU and not totally sure what you should be looking for? Here is everything you need to know about CUDA Cores.},
	language = {en},
	urldate = {2022-06-20},
	journal = {Trusted Reviews},
	author = {Ryles, Gemma},
	month = apr,
	year = {2022},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/6AEZL3DW/what-are-cuda-cores-4226433.html:text/html},
}

@misc{pantigoso_velasquez_performance_2019,
	title = {A performance comparison between {CPU} and {GPU} in {TensorFlow}},
	url = {http://kth.diva-portal.org/smash/record.jsf?pid=diva2%3A1354858&dswid=8907},
	urldate = {2023-03-15},
	author = {Pantigoso Velasquez, Avelin and Lind, Eric},
	year = {2019},
	file = {A performance comparison between CPU and GPU in TensorFlow:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/IPRYYSI3/record.html:text/html},
}

@article{dutta_quantitative_2013,
	title = {Quantitative {Statistical} {Methods} for {Image} {Quality} {Assessment}},
	volume = {3},
	issn = {1838-7640},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3840409/},
	doi = {10.7150/thno.6815},
	abstract = {Quantitative measures of image quality and reliability are critical for both qualitative interpretation and quantitative analysis of medical images. While, in theory, it is possible to analyze reconstructed images by means of Monte Carlo simulations using a large number of noise realizations, the associated computational burden makes this approach impractical. Additionally, this approach is less meaningful in clinical scenarios, where multiple noise realizations are generally unavailable. The practical alternative is to compute closed-form analytical expressions for image quality measures. The objective of this paper is to review statistical analysis techniques that enable us to compute two key metrics: resolution (determined from the local impulse response) and covariance. The underlying methods include fixed-point approaches, which compute these metrics at a fixed point (the unique and stable solution) independent of the iterative algorithm employed, and iteration-based approaches, which yield results that are dependent on the algorithm, initialization, and number of iterations. We also explore extensions of some of these methods to a range of special contexts, including dynamic and motion-compensated image reconstruction. While most of the discussed techniques were developed for emission tomography, the general methods are extensible to other imaging modalities as well. In addition to enabling image characterization, these analysis techniques allow us to control and enhance imaging system performance. We review practical applications where performance improvement is achieved by applying these ideas to the contexts of both hardware (optimizing scanner design) and image reconstruction (designing regularization functions that produce uniform resolution or maximize task-specific figures of merit).},
	number = {10},
	urldate = {2023-03-20},
	journal = {Theranostics},
	author = {Dutta, Joyita and Ahn, Sangtae and Li, Quanzheng},
	month = oct,
	year = {2013},
	pmid = {24312148},
	pmcid = {PMC3840409},
	pages = {741--756},
	file = {PubMed Central Full Text PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/5M4TZMBB/Dutta et al. - 2013 - Quantitative Statistical Methods for Image Quality.pdf:application/pdf},
}

@article{zhou_wang_mean_2009,
	title = {Mean squared error: {Love} it or leave it? {A} new look at {Signal} {Fidelity} {Measures}},
	volume = {26},
	issn = {1053-5888},
	shorttitle = {Mean squared error},
	url = {http://ieeexplore.ieee.org/document/4775883/},
	doi = {10.1109/MSP.2008.930649},
	language = {en},
	number = {1},
	urldate = {2023-03-20},
	journal = {IEEE Signal Processing Magazine},
	author = {{Zhou Wang} and Bovik, A.C.},
	month = jan,
	year = {2009},
	pages = {98--117},
	file = {Zhou Wang and Bovik - 2009 - Mean squared error Love it or leave it A new loo.pdf:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/MUTCU5PQ/Zhou Wang and Bovik - 2009 - Mean squared error Love it or leave it A new loo.pdf:application/pdf},
}

@misc{khalel_sewar_2023,
	title = {Sewar},
	copyright = {MIT},
	url = {https://github.com/andrewekhalel/sewar},
	abstract = {All image quality metrics you need in one package.},
	urldate = {2023-03-27},
	author = {Khalel, Andrew},
	month = mar,
	year = {2023},
	note = {original-date: 2018-08-23T09:11:28Z},
	keywords = {super-resolution, image-processing, quality-metrics},
}

@misc{vasconcelos_leonamtvimage-similarity-scripts_2023,
	title = {leonamtv/image-similarity-scripts},
	url = {https://github.com/leonamtv/image-similarity-scripts},
	urldate = {2023-03-27},
	author = {Vasconcelos, Leonam Teixeira de},
	month = mar,
	year = {2023},
	note = {original-date: 2023-03-21T01:44:08Z},
}

@article{wang_image_2004,
	title = {Image quality assessment: from error visibility to structural similarity},
	volume = {13},
	issn = {1941-0042},
	shorttitle = {Image quality assessment},
	doi = {10.1109/TIP.2003.819861},
	abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	month = apr,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Data mining, Degradation, Humans, Image quality, Indexes, Layout, Quality assessment, Transform coding, Visual perception, Visual system},
	pages = {600--612},
	file = {IEEE Xplore Abstract Record:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/4KTCTEAP/1284395.html:text/html},
}

@article{wang_image_2004-1,
	title = {Image quality assessment: from error visibility to structural similarity},
	volume = {13},
	issn = {1941-0042},
	shorttitle = {Image quality assessment},
	doi = {10.1109/TIP.2003.819861},
	abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	month = apr,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Data mining, Degradation, Humans, Image quality, Indexes, Layout, Quality assessment, Transform coding, Visual perception, Visual system},
	pages = {600--612},
	file = {IEEE Xplore Abstract Record:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/33GY2ZHI/1284395.html:text/html},
}

@article{wang_universal_2002,
	title = {A universal image quality index},
	volume = {9},
	issn = {1558-2361},
	doi = {10.1109/97.995823},
	abstract = {We propose a new universal objective image quality index, which is easy to calculate and applicable to various image processing applications. Instead of using traditional error summation methods, the proposed index is designed by modeling any image distortion as a combination of three factors: loss of correlation, luminance distortion, and contrast distortion. Although the new index is mathematically defined and no human visual system model is explicitly employed, our experiments on various image distortion types indicate that it performs significantly better than the widely used distortion metric mean squared error. Demonstrative images and an efficient MATLAB implementation of the algorithm are available online at http://anchovy.ece.utexas.edu//spl sim/zwang/research/quality\_index/demo.html.},
	number = {3},
	journal = {IEEE Signal Processing Letters},
	author = {Wang, Zhou and Bovik, A.C.},
	month = mar,
	year = {2002},
	note = {Conference Name: IEEE Signal Processing Letters},
	keywords = {PSNR, Humans, Image quality, Visual system, Distortion measurement, Dynamic range, Image processing, Mathematical model, Signal to noise ratio, Testing},
	pages = {81--84},
	file = {IEEE Xplore Abstract Record:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/PV6FB292/995823.html:text/html},
}

@inproceedings{wang_multiscale_2003,
	title = {Multiscale structural similarity for image quality assessment},
	volume = {2},
	doi = {10.1109/ACSSC.2003.1292216},
	abstract = {The structural similarity image quality paradigm is based on the assumption that the human visual system is highly adapted for extracting structural information from the scene, and therefore a measure of structural similarity can provide a good approximation to perceived image quality. This paper proposes a multiscale structural similarity method, which supplies more flexibility than previous single-scale methods in incorporating the variations of viewing conditions. We develop an image synthesis method to calibrate the parameters that define the relative importance of different scales. Experimental comparisons demonstrate the effectiveness of the proposed method.},
	booktitle = {The {Thrity}-{Seventh} {Asilomar} {Conference} on {Signals}, {Systems} \& {Computers}, 2003},
	author = {Wang, Z. and Simoncelli, E.P. and Bovik, A.C.},
	month = nov,
	year = {2003},
	keywords = {Data mining, Humans, Image quality, Layout, Visual system, Distortion measurement, Displays, Electric variables measurement, Optical filters, Signal processing},
	pages = {1398--1402 Vol.2},
	file = {IEEE Xplore Abstract Record:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/APXBZF5T/1292216.html:text/html;Submitted Version:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/YAKV878W/Wang et al. - 2003 - Multiscale structural similarity for image quality.pdf:application/pdf},
}

@inproceedings{wald_quality_2000,
	title = {Quality of high resolution synthesised images: {Is} there a simple criterion ?},
	shorttitle = {Quality of high resolution synthesised images},
	url = {https://hal.science/hal-00395027},
	abstract = {Methods exist which are synthesizing high spatial resolution high spectral content images from a set of low spatial resolution high spectral content images and high spatial resolution low spectral content images. There is a need to quantify the quality of these synthesized images for both the producers and customers of such fused products. Some protocols are discussed. The main concern is on the definition of a parameter characterizing the quality. The requirements on this parameter are discussed. A review of published parameters is made and a new one is proposed. From published works, it is shown that a good quality is achieved when the parameter is less than 3.},
	language = {en},
	urldate = {2023-03-27},
	publisher = {SEE/URISCA},
	author = {Wald, Lucien},
	month = jan,
	year = {2000},
	pages = {99},
	file = {Full Text PDF:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/GX99CKCW/Wald - 2000 - Quality of high resolution synthesised images Is .pdf:application/pdf},
}

@misc{nvidia_download_2023,
	title = {Download the latest official {NVIDIA} drivers},
	url = {https://www.nvidia.com/download/index.aspx},
	abstract = {Download the latest official NVIDIA drivers},
	language = {en-us},
	urldate = {2023-04-02},
	author = {NVIDIA, NVIDIA},
	month = apr,
	year = {2023},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/UJLJRU76/index.html:text/html},
}

@misc{nvidia_advanced_2023,
	title = {Advanced {Driver} {Search} official {NVIDIA} drivers},
	url = {https://www.nvidia.com/Download/Find.aspx},
	abstract = {Advanced Driver Search official NVIDIA drivers},
	language = {en-us},
	urldate = {2023-04-02},
	author = {NVIDIA, NVIDIA},
	month = feb,
	year = {2023},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/GTXBHY7V/Find.html:text/html},
}

@misc{pytorch_pytorch_2023,
	title = {{PyTorch} documentation — {PyTorch} 2.0 documentation},
	url = {https://pytorch.org/docs/stable/index.html},
	urldate = {2023-04-09},
	author = {Pytorch, Pytorch},
	month = apr,
	year = {2023},
	file = {PyTorch documentation — PyTorch 2.0 documentation:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/4N52VBCC/index.html:text/html},
}

@misc{karpathy_pytorch_2019,
	title = {{PyTorch} at {Tesla} - {Andrej} {Karpathy}, {Tesla}},
	url = {https://www.youtube.com/watch?v=oBklltKXtDE},
	abstract = {Hear from Andrej Karpathy on how Tesla is using PyTorch to develop full self-driving capabilities for its vehicles, including AutoPilot and Smart Summon.},
	urldate = {2023-04-09},
	author = {Karpathy, Andrej},
	month = nov,
	year = {2019},
}

@misc{zemlin_welcoming_nodate,
	title = {Welcoming {PyTorch} to the {Linux} {Foundation} - {Linux} {Foundation}},
	url = {https://www.linuxfoundation.org/blog/blog/welcoming-pytorch-to-the-linux-foundation},
	abstract = {Today we are more than thrilled to welcome PyTorch to the Linux Foundation. Honestly, it’s hard to capture how big a deal this is for us in a single post but I’ll try.  TL;DR — PyTorch is one of the most important and successful machine learning software projects in the world today. We are excited […]},
	language = {en},
	urldate = {2023-04-09},
	author = {Zemlin, Jim},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/YPG53BW8/welcoming-pytorch-to-the-linux-foundation.html:text/html},
}

@misc{goodman_uber_2017,
	title = {Uber {AI} {Labs} {Open} {Sources} {Pyro}, a {Deep} {Probabilistic} {Programming} {Language}},
	url = {https://www.uber.com/en-GR/blog/pyro/},
	abstract = {Pyro is an open source probabilistic programming language that unites modern deep learning with Bayesian modeling for a tool-first approach to AI.},
	urldate = {2023-04-09},
	journal = {Uber Blog},
	author = {Goodman, Noah},
	month = nov,
	year = {2017},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/4ANMIQ5G/pyro.html:text/html},
}

@misc{tensorflow_why_2023,
	title = {Why {TensorFlow}},
	url = {https://www.tensorflow.org/about},
	abstract = {Whether you're an expert or a beginner, TensorFlow is an end-to-end platform that makes it easy for you to build and deploy ML models.},
	language = {en},
	urldate = {2023-04-09},
	journal = {TensorFlow},
	author = {Tensorflow, Tensorflow},
	month = apr,
	year = {2023},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/MS67N7VN/about.html:text/html},
}

@misc{jeff_tensorflow_2015,
	title = {{TensorFlow} - {Google}’s latest machine learning system, open sourced for everyone},
	url = {https://ai.googleblog.com/2015/11/tensorflow-googles-latest-machine.html},
	language = {en},
	urldate = {2023-04-09},
	author = {Jeff, Dean and Monga, Rajat},
	month = nov,
	year = {2015},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/7C68HF3T/tensorflow-googles-latest-machine.html:text/html},
}

@article{abadi_tensorflow_2015,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be
executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones
and tablets up to large-scale distributed systems of hundreds
of machines and thousands of computational devices such as
GPU cards. The system is flexible and can be used to express
a wide variety of algorithms, including training and inference
algorithms for deep neural network models, and it has been
used for conducting research and for deploying machine learning systems into production across more than a dozen areas of
computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural
language processing, geographic information extraction, and
computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that
we have built at Google. The TensorFlow API and a reference
implementation were released as an open-source package under
the Apache 2.0 license in November, 2015 and are available at
www.tensorflow.org.},
	language = {en},
	author = {Abadi, Martın and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = nov,
	year = {2015},
	file = {Abadi et al. - TensorFlow Large-Scale Machine Learning on Hetero.pdf:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/Y6HGIZLI/Abadi et al. - TensorFlow Large-Scale Machine Learning on Hetero.pdf:application/pdf},
}

@misc{davies_complete_2020,
	title = {A {Complete} {Guide} to the {Google} {RankBrain} {Algorithm}},
	url = {https://www.searchenginejournal.com/google-algorithm-history/rankbrain/},
	abstract = {What is Google RankBrain? How does it work? Can you optimize for it? Here's everything you need to know about Google's RankBrain algorithm.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Search Engine Journal},
	author = {Davies, Dave and Read, 15 Min and Shares, 3 5k and Reads, 95k},
	month = sep,
	year = {2020},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/A9G6CDY4/rankbrain.html:text/html},
}

@misc{ngahane_winding_2019,
	title = {The {Winding} {Road} to {Better} {Machine} {Learning} {Infrastructure} {Through} {Tensorflow} {Extended} and {Kubeflow}},
	url = {https://engineering.atspotify.com/2019/12/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/},
	abstract = {The Winding Road to Better Machine Learning Infrastructure Through Tensorflow Extended and Kubeflow - Spotify Engineering},
	language = {en-US},
	urldate = {2023-04-10},
	journal = {Spotify Engineering},
	author = {Ngahane, Samuel and Baer, Josh},
	month = dec,
	year = {2019},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/CPVNECIA/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kube.html:text/html},
}

@misc{lenail_nn_2023,
	title = {{NN} {SVG}},
	url = {https://alexlenail.me/NN-SVG/},
	urldate = {2023-04-10},
	author = {Lenail, Alexander},
	month = apr,
	year = {2023},
	file = {NN SVG:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/RHB5643A/NN-SVG.html:text/html},
}

@misc{chen_nash_2022,
	title = {Nash {Equilibrium}: {How} {It} {Works} in {Game} {Theory}, {Examples}, {Plus} {Prisoner}’s {Dilemma}},
	shorttitle = {Nash {Equilibrium}},
	url = {https://www.investopedia.com/terms/n/nash-equilibrium.asp},
	abstract = {Nash equilibrium is a game theory concept where optimal outcome is when there is no incentive for players to deviate from their initial strategy.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Investopedia},
	author = {Chen, James},
	month = nov,
	year = {2022},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/QRH72IMF/nash-equilibrium.html:text/html},
}

@misc{eldridge_nash_2022,
	title = {Nash equilibrium {\textbar} {Definition}, {Examples}, \& {Facts} {\textbar} {Britannica}},
	url = {https://www.britannica.com/science/Nash-equilibrium},
	abstract = {Nash equilibrium, also called Nash solution, in game theory, an outcome in a noncooperative game for two or more players in which no player’s expected outcome can be improved by changing one’s own strategy. The Nash equilibrium is a key concept in game theory, in which it defines the solution of N-player noncooperative games. It is named for American mathematician John Nash, who was awarded the 1994 Nobel Prize for Economics for his contributions to game theory. Game theory uses mathematics to model and analyze situations in which decisions are interdependent. While it can be used to model recreational games},
	language = {en},
	urldate = {2023-04-10},
	author = {Eldridge, Stephen},
	month = dec,
	year = {2022},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/QHR5SQGN/Nash-equilibrium.html:text/html},
}

@misc{kitson_installing_2022,
	title = {Installing {Tensorflow} with {CUDA}, {cuDNN} and {GPU} support on {Windows} 10},
	url = {https://towardsdatascience.com/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781},
	abstract = {Pimp Up your PC for Deep Learning — Part 2},
	language = {en},
	urldate = {2024-03-07},
	journal = {Medium},
	author = {Kitson, Joanne},
	month = aug,
	year = {2022},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/APAMU345/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781.html:text/html},
}

@misc{blueamulet_blueamuletbasicsr_2023,
	title = {{BlueAmulet}/{BasicSR}},
	copyright = {Apache-2.0},
	url = {https://github.com/BlueAmulet/BasicSR},
	abstract = {Fork of Basic Super-Resolution codes for development. Includes ESRGAN, SFT-GAN for training and testing.},
	urldate = {2024-03-17},
	author = {BlueAmulet},
	month = jul,
	year = {2023},
	note = {original-date: 2019-07-09T16:00:14Z},
	keywords = {basicsr, esrgan, image-upscaling, super-resolution, upscaler, upscaling},
}


@misc{dicom_standard_history_2019,
	title = {History},
	url = {https://www.dicomstandard.org/history},
	language = {en},
	urldate = {2024-07-10},
	journal = {DICOM},
	author = {{DICOM Standard}},
	year = {2019},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/62EGN5BK/history.html:text/html},
}

@misc{medical_connections_dicomobjects_2011,
	title = {{DicomObjects} {\textbar} {DICOM} {API} / {SDK} {Toolkit}},
	url = {https://www.medicalconnections.co.uk/DicomObjects/},
	abstract = {Overview DicomObjects is a well established and tested API / SDK Toolkit library for developers new to DICOM.},
	language = {en-us},
	urldate = {2024-07-10},
	author = {{Medical Connections}},
	month = aug,
	year = {2011},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/CBVN4CD6/DicomObjects.html:text/html},
}

@misc{medical_connections_dicom_2007,
	title = {{DICOM} {DateTime} {Format}},
	url = {https://www.medicalconnections.co.uk/kb/DICOM-DateTime-Format/},
	abstract = {The Standard DICOM DateTime Format The standard DateTime format (YYYYMMDD) is explained in Table 6.},
	language = {en-us},
	urldate = {2024-07-10},
	author = {{Medical Connections}},
	month = jan,
	year = {2007},
	note = {Section: kb},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/D7TF9ZMM/DICOM-DateTime-Format.html:text/html},
}

@misc{dicom_standard_current_2024,
	title = {Current {Edition}},
	url = {https://www.dicomstandard.org/current},
	language = {en},
	year = {2024},
	urldate = {2024-07-10},
	journal = {DICOM},
	author = {{DICOM Standard}},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/DZYUPKLD/current.html:text/html},
}

@misc{weston_understanding_2020,
	title = {Understanding {DICOM}},
	url = {https://towardsdatascience.com/understanding-dicom-bce665e62b72},
	abstract = {How to read, write, and organize medical images},
	language = {en},
	urldate = {2024-07-10},
	journal = {Medium},
	author = {Weston, Alexander},
	month = oct,
	year = {2020},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/AIFCRN9G/understanding-dicom-bce665e62b72.html:text/html},
}


@misc{nvidia_cuda_2024,
	title = {{CUDA} {Toolkit} 12.1 {Downloads}},
	shorttitle = {{CUDA} {Downloads}},
	url = {https://developer.nvidia.com/cuda-downloads},
	abstract = {Get the latest feature updates to NVIDIA's proprietary compute stack.},
	language = {en-US},
	urldate = {2024-07-14},
	journal = {NVIDIA Developer},
	author = {{NVIDIA}},
	month = jul,
	year = {2024},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/UCXEYVQE/cuda-downloads.html:text/html},
}


@misc{srivastava_astronomy_2024,
	title = {Astronomy {Image} {Classification} {Dataset}},
	url = {https://www.kaggle.com/datasets/abhikalpsrivastava15/space-images-category},
	abstract = {Kaggle is the world’s largest data science community with powerful tools and resources to help you achieve your data science goals.},
	language = {en},
	urldate = {2024-07-14},
	author = {Srivastava, Abhikalp},
	month = jul,
	year = {2024},
	file = {Snapshot:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/YE3HRZAT/space-images-category.html:text/html},
}

@misc{sdss_sloan_2024,
	title = {Sloan {Digital} {Sky} {Survey}},
	url = {https://www.sdss4.org/},
	urldate = {2024-07-14},
	author = {{SDSS}},
	month = jul,
	year = {2024},
	file = {SDSS:/home/leonamtv/snap/zotero-snap/common/Zotero/storage/9GN5WVEE/www.sdss4.org.html:text/html},
}
